{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Umang_NLP_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBXDVamzQjJj"
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut,KFold, StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "from random import shuffle\n",
        "import itertools as tools\n",
        "import tensorflow as tf\n",
        "import pickle as pkl\n",
        "from os import walk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import gc\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import math\n",
        "import random\n",
        "import collections\n",
        "import os,glob\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH8UWsUgpwbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bdd2e6-7d47-4d55-db10-f198b4aa92c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW0qppx2qRpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5714a6d1-a975-4106-d106-692eb3a855d4"
      },
      "source": [
        "!pip3 install indic-nlp-library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TFVeyllatf"
      },
      "source": [
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp import common\n",
        "#common.INDIC_RESOURCES_PATH=\"./ExternalDependencies/indic_nlp_resources\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edNistsylato"
      },
      "source": [
        "embeddings_path = './drive/MyDrive/Embeddings/embeddings'\n",
        "dataset_path = './drive/MyDrive/Embeddings/dataset_unbalanced.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2pvbKl3SHLo"
      },
      "source": [
        "class Preprocessor:\n",
        "    \n",
        "    def __init__(self, embidding_dims= 100, max_sequence_length = 100, padding_type='post'):        \n",
        "        self.word_index_len = None\n",
        "        self.embeddings_matrix = None\n",
        "        self.embedding_dim = embidding_dims\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.padding_type = padding_type \n",
        "        self.embeddings_dict = None    \n",
        "\n",
        "    # function to generate embedding matrix\n",
        "    def make_embeddings(self, path_to_embeddings= embeddings_path):            \n",
        "        embeddings_index = {}\n",
        "        with open(f'{path_to_embeddings}', encoding=\"utf8\") as f:\n",
        "            count=0\n",
        "            for line in f:\n",
        "              values = line.split()\n",
        "              count+=1\n",
        "              if (count==1265895 or count==1):\n",
        "                continue\n",
        "              try:                \n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                if (len(coefs)!=100):\n",
        "                  continue\n",
        "                embeddings_index[word] = coefs\n",
        "              except:\n",
        "                print(count)\n",
        "                print(values)\n",
        "        self.word_index_len=len(embeddings_index)\n",
        "        self.embeddings_dict={}\n",
        "        embedding_index = np.zeros((self.word_index_len+1, self.embedding_dim))\n",
        "        i=1\n",
        "        for word in embeddings_index:\n",
        "            embedding_vector = embeddings_index[word]\n",
        "            self.embeddings_dict[word]=i\n",
        "            if embedding_vector is not None:\n",
        "                embedding_index[i] = embedding_vector\n",
        "            i+=1\n",
        "\n",
        "        self.embeddings_matrix = embedding_index\n",
        "\n",
        "    # function to convert sentences to vectors\n",
        "    def vectorize_sentences(self,sentences):\n",
        "        vectors=[]\n",
        "        for sentence in sentences:\n",
        "            vector=np.zeros(shape=(100))\n",
        "            words=indic_tokenize.trivial_tokenize(sentence)\n",
        "            count=0\n",
        "            for word in words:\n",
        "              if word in self.embeddings_dict:                \n",
        "                vector[count]=self.embeddings_dict[word]\n",
        "              count+=1\n",
        "              if (count==100):\n",
        "                break\n",
        "            vectors.append(vector)\n",
        "        return vectors\n",
        "        \n",
        "          \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzZh1dRdtCfe"
      },
      "source": [
        "### Model: includes all model related functionalities\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFLb-J1LtQ7q"
      },
      "source": [
        "Similarity matrix:\n",
        "trainable matrix M that captures similarity between two sentences according to the equation: \n",
        "\n",
        " ![similarity function](https://drive.google.com/uc?id=1y_ojFiDHkrbOwi7LEXrGo2UTXo4Qbv5o)\n",
        "\n",
        "where: \n",
        "*   Xf: first sentence\n",
        "*   Xs: second sentence\n",
        "*   M: similarity matrix (trainable weights)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lApxKxBOU5OF"
      },
      "source": [
        "# defining similarity layer by subclassing keras layer\n",
        "\n",
        "class SimilarityMatrix(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,dims, **kwargs):\n",
        "        self.dims_length, self.dims_width = dims\n",
        "        super(SimilarityMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self._m = self.add_weight(name='M', \n",
        "                                    shape=(self.dims_length,self.dims_width),\n",
        "                                    initializer='uniform',\n",
        "                                    trainable=True)\n",
        "        super(SimilarityMatrix, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, y): \n",
        "        xf, xs = y\n",
        "        sim1=tf.matmul(xf, self._m)\n",
        "        transposed = tf.reshape(K.transpose(xs),(-1, 100, 1))\n",
        "        sim2=tf.matmul(sim1, transposed)\n",
        "        return sim2\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (1)\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'dims_length': self.dims_length, \n",
        "            'dims_width': self.dims_width\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB9tzXpzv03g"
      },
      "source": [
        "Model Helper: includes all training related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFdnW67fpYPJ"
      },
      "source": [
        "class ModelHelper:\n",
        "  \n",
        "    @staticmethod\n",
        "    def negative_log_likelihood(y_true, y_pred):\n",
        "        return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_ROC(y_true, y_predictions, title=''):\n",
        "        ## calculate the FPR, TPR, Thresholds and AUC value\n",
        "        false_pos_rate, true_pos_rate, thresholds = roc_curve(y_true, y_predictions)\n",
        "        auc_val = auc(false_pos_rate, true_pos_rate)\n",
        "\n",
        "        ## plot ROC curve\n",
        "        plt.figure(1)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.plot(false_pos_rate, true_pos_rate, label=f'{title}' +' (area = {:.3f})'.format(auc_val))\n",
        "        plt.xlabel('False positive rate')\n",
        "        plt.ylabel('True positive rate')\n",
        "        plt.title('ROC curve')\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def compile_model(model , loss_func, monitor_metrics = ['acc'], optimizer='adam'):\n",
        "        model.compile(optimizer=optimizer, loss=loss_func, metrics=monitor_metrics)   \n",
        "\n",
        "    @staticmethod\n",
        "    def train_model_kfolds(data, model_class, loss_func, num_of_folds, verbose=2, batch_size=128, plot_roc = False, plot_prec_recall = False ):\n",
        "        \n",
        "        model_callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(patience=8),\n",
        "        ]\n",
        "        \n",
        "        X_data, y_data = data[0].astype(np.float32), data[1].astype(np.float32)\n",
        "        \n",
        "        count = 0\n",
        "\n",
        "        for train_index, test_index in StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=42).split(X_data, y_data):\n",
        "            \n",
        "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
        "            \n",
        "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
        "            \n",
        "            model = model_class()\n",
        "            model.make_model()\n",
        "            model = model.model\n",
        "\n",
        "            ModelHelper.compile_model(model, ModelHelper.negative_log_likelihood)\n",
        "\n",
        "            model.fit(X_train,y_train,validation_data=(X_test,y_test),verbose=verbose,epochs=100, batch_size=batch_size, callbacks=model_callbacks)\n",
        "            \n",
        "\n",
        "            pred = model.predict(X_test).ravel()\n",
        "            \n",
        "            loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "            print(f'fold #{count+1} test loss: {loss}, test acc: {acc}')\n",
        "            \n",
        "            if plot_roc:\n",
        "              ModelHelper.plot_ROC(y_test, pred, 'test data')\n",
        "\n",
        "            count += 1\n",
        "            gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPij43Uev_k6"
      },
      "source": [
        "Model: includes all the CNN related functions\n",
        "the make_model function builds a Convolutional Neural Net according to the architecture suggested by the paper as shown in the figure below\n",
        "\n",
        "![cnn model architecture](https://drive.google.com/uc?id=118Olwuh9VL5_Rt_IBg6G5JfuT2KEl-Z5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZu6Ahs_SFto"
      },
      "source": [
        "class Model:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.num_of_folds = int(5)\n",
        "        self.dataset = None\n",
        "        self.data = None\n",
        "        self.model = None\n",
        "        self.test_data = None\n",
        "        \n",
        "        self.preprocessor = Preprocessor()\n",
        "        self.preprocessor.make_embeddings()  \n",
        "    \n",
        "    def make_model(self):\n",
        "\n",
        "        X_input =  tf.keras.Input(shape=(3, 100), name=\"input-sentences\")\n",
        "        \n",
        "        \n",
        "        embedding_layer = tf.keras.layers.Embedding(input_dim= self.preprocessor.word_index_len+1, \n",
        "                                                    output_dim=self.preprocessor.embedding_dim, \n",
        "                                                    input_length=self.preprocessor.max_sequence_length,\n",
        "                                                    trainable = False,\n",
        "                                                    name='fasttext-embedding-layer')\n",
        "        embedding_layer.build((None,))\n",
        "        embedding_layer.set_weights([self.preprocessor.embeddings_matrix])\n",
        "        \n",
        "        first_sentence =  embedding_layer(X_input[:,0,:])\n",
        "        second_sentence =  embedding_layer(X_input[:,1,:])\n",
        "        third_sentence =  embedding_layer(X_input[:,2,:])\n",
        "        \n",
        "        convolutional_filters_map = tf.keras.layers.Conv1D(100,kernel_size=(3), activation='relu', use_bias=True, name='features-map')\n",
        "        \n",
        "        Xf = convolutional_filters_map(first_sentence)\n",
        "        Xs = convolutional_filters_map(second_sentence)         \n",
        "        Xt = convolutional_filters_map(third_sentence)   \n",
        "\n",
        "\n",
        "        Xf = tf.keras.layers.MaxPool1D(98, name='first-sentence-pool')(Xf)\n",
        "        Xs = tf.keras.layers.MaxPool1D(98, name='second-sentence-pool')(Xs)\n",
        "        Xt = tf.keras.layers.MaxPool1D(98, name='third-sentence-pool')(Xt)\n",
        "\n",
        "        similarity_fnc = SimilarityMatrix((100,100))\n",
        "\n",
        "        sim_fs = similarity_fnc([Xf, Xs])\n",
        "        sim_st = similarity_fnc([Xs, Xt])\n",
        "\n",
        "        X = tf.keras.layers.concatenate([Xf, sim_fs, Xs, sim_st, Xt])\n",
        "\n",
        "        X = tf.keras.layers.Dense(256, activation='relu', name='fc1', use_bias=True)(X)\n",
        "        X = tf.keras.layers.Dropout(0.333)(X)\n",
        "\n",
        "        X = tf.keras.layers.Dense(512, activation='relu', name='fc2', use_bias=True)(X)\n",
        "        X = tf.keras.layers.Dropout(0.333)(X)\n",
        "\n",
        "        X = tf.keras.layers.Dense(512, activation='relu', name='fc3', use_bias=True)(X)\n",
        "        X = tf.keras.layers.Dropout(0.333)(X)\n",
        "\n",
        "        X = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(X)\n",
        "\n",
        "        model = tf.keras.Model(inputs=[X_input], outputs=[X])\n",
        "\n",
        "        self.model = model\n",
        "    \n",
        "    def load_data_from_csv(self, data_path, separator=',', split_train_test=False, make_balanced=False):\n",
        "        \n",
        "        '''\n",
        "            load data from CSV file into dataframe\n",
        "            \n",
        "            -- inputs:\n",
        "                data_path: path to file where data is saved\n",
        "                separator (optional): value seprator to the file, default is comma\n",
        "        '''\n",
        "        \n",
        "        self.data = pd.read_csv(f'{data_path}', sep=',')\n",
        "        self.data['data'] = self.data['data'].apply(lambda x: np.array(sentence_tokenize.sentence_split(x,lang='hi')[:3]))\n",
        "        self.data['data'] = self.data['data'].apply(lambda x: np.array(self.preprocessor.vectorize_sentences(x)))\n",
        "\n",
        "        if make_balanced:\n",
        "          freq = list(self.data['label'].value_counts())\n",
        "          freq = freq[0]//freq[1]-1\n",
        "          \n",
        "          df_coherent = self.data.loc[self.data['label'] == 1]\n",
        "          df_coherent_replecated = pd.concat([df_coherent]*freq, ignore_index=True)\n",
        "          self.data = pd.concat([df_coherent_replecated, self.data], ignore_index=True)\n",
        "        \n",
        "        data_list = self.data['data'].values.tolist()\n",
        "        label_list = self.data['label'].values.tolist()\n",
        "\n",
        "        final_data = []\n",
        "        final_labels = []\n",
        "        for i in range(len(data_list)):\n",
        "          data_entry = data_list[i]\n",
        "          if data_entry.shape[0] == 3:\n",
        "            final_data.append(data_entry)\n",
        "            final_labels.append(label_list[i])\n",
        "\n",
        "        \n",
        "        self.data = (np.array(final_data), np.array(final_labels).reshape(-1,1))\n",
        "      \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mszFz09p_J6"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckul1uZJTIH3",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "m = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCbZ3zWfvIyA"
      },
      "source": [
        "m.load_data_from_csv(dataset_path)\n",
        "unique_elements, counts_elements = np.unique(m.data[1], return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAZxEk-1xU6e"
      },
      "source": [
        "m.load_data_from_csv(dataset_path, make_balanced=True)\n",
        "unique_elements, counts_elements = np.unique(m.data[1], return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1gbF4MLfngo"
      },
      "source": [
        "m.make_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnlDGFhcvL3q"
      },
      "source": [
        "ModelHelper.train_model_kfolds(m.data, Model, ModelHelper.negative_log_likelihood, m.num_of_folds, plot_roc=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3jBhFm4jgR2"
      },
      "source": [
        "m.model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0QkKdPR7gQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}